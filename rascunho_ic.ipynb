{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laisab/IC/blob/main/rascunho_ic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oprVIBIKdGE",
        "outputId": "49bce39f-e434-48dc-d8e1-f1c7c859ea45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "MqQOLcBbKeRN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dicionário com datasets e seus respectivos targets\n",
        "dicio_datasets = {\n",
        "    'Adult.csv': 'income'\n",
        "}"
      ],
      "metadata": {
        "id": "U6ypeFU9KfTr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idade = {'age': {\n",
        "    'bins': [17, 30, 60, np.inf],\n",
        "    'labels': ['jovem', 'adulto', 'idoso']\n",
        "}}"
      ],
      "metadata": {
        "id": "i2o82MY9bPbx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dicionário com datasets e seus respectivos atributos sensíveis\n",
        "dicio_atributos_sensiveis = {\n",
        "    'Adult.csv': {\n",
        "        'age': {\n",
        "            'bins': [17, 30, 60, np.inf],\n",
        "            'labels': ['jovem', 'adulto', 'idoso']\n",
        "        },\n",
        "        'education': None,\n",
        "        'education.num': None,\n",
        "        'marital.status': None,\n",
        "        'relationships': None,\n",
        "        'race': None,\n",
        "        'sex': None,\n",
        "        'native.country': None\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "RaZ9cpg_RvP_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caminho_drive = '/content/drive/MyDrive/IC/Datasets/'\n",
        "novo_caminho = '/content/drive/MyDrive/Teste/'"
      ],
      "metadata": {
        "id": "1OKyDvkUKght"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separar_feature_target(caminho_completo):\n",
        "\n",
        "  try:\n",
        "    df = pd.read_csv(caminho_completo)\n",
        "\n",
        "    # Obtém o nome do target\n",
        "    nome_arquivo = os.path.basename(caminho_completo)\n",
        "    nome_target = dicio_datasets.get(nome_arquivo)\n",
        "\n",
        "    if not nome_target:\n",
        "      raise ValueError(f'Coluna target não encontrada para o arquivo {nome_arquivo}')\n",
        "\n",
        "    # Separa features (X) e target (y)\n",
        "    X = df.drop(columns=[nome_target])\n",
        "    y = df[nome_target]\n",
        "\n",
        "    # Dicionário para armazenar as colunas OneHotEncoder\n",
        "    X_enc = {}\n",
        "\n",
        "    # Codificação de features categóricas com OneHotEncoder\n",
        "    colunas_categoricas = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "    if colunas_categoricas.size > 0: # Verificação de colunas categóricas para evitar erros com datasets numéricos\n",
        "      enc = OneHotEncoder(handle_unknown='ignore') # Lida com categorias desconhecidas em dados futuros\n",
        "      X_encoded = enc.fit_transform(X[colunas_categoricas]).toarray()\n",
        "      nome_features_categoricas = enc.get_feature_names_out(colunas_categoricas)\n",
        "      X_encoded_df = pd.DataFrame(X_encoded, columns=nome_features_categoricas)\n",
        "\n",
        "      # Para cada coluna categórica original, guarda a lista das novas colunas geradas\n",
        "      for i, coluna_original in enumerate(colunas_categoricas):\n",
        "        X_enc[coluna_original] = [nome_atributo for nome_atributo in nome_features_categoricas if nome_atributo.startswith(f'{coluna_original}')]\n",
        "\n",
        "      X = X.select_dtypes(exclude=['object']) # Remove as colunas categóricas originais\n",
        "      X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "    return X, y, X_enc\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f'\\nErro ao separar os dados de {nome_arquivo}: {e}')\n",
        "    return None"
      ],
      "metadata": {
        "id": "kEzz5A6PKmAs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predição dos datasets\n",
        "def predizer_dataset(modelo, X, y, cv):\n",
        "  try:\n",
        "    y_pred = cross_val_predict(modelo, X, y, cv=cv, method='predict')\n",
        "    y_prob = cross_val_predict(modelo, X, y, cv=cv, method='predict_proba')\n",
        "    return y_pred, y_prob\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f'\\nErro ao predizer dataset: {e}')\n",
        "    return None"
      ],
      "metadata": {
        "id": "e9ew7ZZYL-F5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação dos datasets\n",
        "def avaliar_dataset(y, y_pred, y_prob):\n",
        "  try:\n",
        "    metricas = {\n",
        "      'accuracy': accuracy_score,\n",
        "      'f1_score': lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro'),\n",
        "      'roc_auc_score': lambda y_true, y_pred_proba: roc_auc_score(y_true, y_pred_proba, average='macro', multi_class='ovr') if len(np.unique(y_true)) > 2 else roc_auc_score(y_true, y_pred_proba[:, 1])\n",
        "    }\n",
        "\n",
        "    avaliacao = {}\n",
        "    for metrica, metrica_funcao in metricas.items():\n",
        "      try:\n",
        "        if metrica == 'roc_auc_score':\n",
        "          avaliacao[metrica] = metrica_funcao(y, y_prob)\n",
        "        else:\n",
        "          avaliacao[metrica] = metrica_funcao(y, y_pred)\n",
        "      except Exception as e:\n",
        "        print(f'\\nErro ao calcular {metrica}: {e}')\n",
        "        avaliacao[metrica] = None\n",
        "\n",
        "    return avaliacao\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f'\\nErro ao avaliar dataset: {e}')\n",
        "    return None"
      ],
      "metadata": {
        "id": "IXmQpd-IOeGA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação dos segmentos\n",
        "def avaliar_segmentos(X, y, predicao, probabilidade, X_enc, avaliar_dataset_func, dataset=\"\", atributos_sensiveis=None):\n",
        "  print(f'AVALIAÇÃO DOS SEGMENTOS: {dataset}')\n",
        "\n",
        "  for coluna_original in atributos_sensiveis:\n",
        "    if coluna_original not in X.columns and coluna_original not in X_enc:\n",
        "      print(f'Atributo sensível {coluna_original} não encontrado para {dataset}')\n",
        "      continue\n",
        "\n",
        "    print(f'\\n Subgrupos para o atributo {coluna_original}')\n",
        "\n",
        "    if coluna_original in X_enc:\n",
        "      colunas_atributos_sensiveis = X_enc[coluna_original]\n",
        "\n",
        "      for coluna_atributo in colunas_atributos_sensiveis:\n",
        "        idx = X[coluna_atributo] == 1\n",
        "\n",
        "        if idx.sum() > 0:\n",
        "          subset_y = y[idx]\n",
        "          subset_predicao = predicao[idx]\n",
        "          subset_probabilidade = probabilidade[idx]\n",
        "\n",
        "          nome_categorico = coluna_atributo.replace(f'{coluna_original}_', '')\n",
        "          print(f'  {nome_categorico}: {avaliar_dataset_func(subset_y, subset_predicao, subset_probabilidade)}')\n",
        "        else:\n",
        "          nome_categorico = coluna_atributo.replace(f'{coluna_original}_', '')\n",
        "          print(f'  {nome_categorico}: Nenhum registro encontrado')\n",
        "    else:\n",
        "      valor_unico = X[coluna_original].unique()\n",
        "\n",
        "      if X[coluna_original].dtype != 'object' and len(valor_unico) > 50:\n",
        "        print(f'  {coluna_original} é numérica e possui muitos valores.')\n",
        "        continue\n",
        "\n",
        "      for valor in valor_unico:\n",
        "        idx = X[coluna_original] == valor\n",
        "\n",
        "        if idx.sum() > 0:\n",
        "          subset_y = y[idx]\n",
        "          subset_predicao = predicao[idx]\n",
        "          subset_probabilidade = probabilidade[idx]\n",
        "\n",
        "          print(f'  {valor}: {avaliar_dataset_func(subset_y, subset_predicao, subset_probabilidade)}')\n",
        "        else:\n",
        "          print(f'  {valor}: Nenhum registro encontrado')"
      ],
      "metadata": {
        "id": "Fgh2S9VJSGnp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista os arquivos csv na pasta\n",
        "arquivos = [f for f in os.listdir(caminho_drive) if f.endswith('.csv') and f in dicio_datasets]\n",
        "arquivos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkFNkrv-MNG5",
        "outputId": "5ab391a2-3242-42e6-cf13-ebd7a42318f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adult.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Processamento de cada dataset\n",
        "resultados = []\n",
        "res_metrica_tabela = []\n",
        "\n",
        "modelo = RandomForestClassifier(n_estimators=20, random_state=42)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for arquivo in arquivos:\n",
        "  caminho_completo = os.path.join(caminho_drive, arquivo)\n",
        "  dados = separar_feature_target(caminho_completo)\n",
        "\n",
        "  if dados is not None:\n",
        "    X, y, X_enc = dados\n",
        "    predicao, probabilidade = predizer_dataset(modelo, X, y, cv)\n",
        "    resultados = avaliar_dataset(y, predicao, probabilidade)\n",
        "\n",
        "    print(resultados)\n",
        "\n",
        "    res_metrica_tabela.append([\n",
        "      arquivo,\n",
        "      modelo,\n",
        "      resultados.get('accuracy'),\n",
        "      resultados.get('f1_score'),\n",
        "      resultados.get('roc_auc_score')\n",
        "    ])\n",
        "\n",
        "    arquivo_pred = f\"Predicao_{arquivo}\"\n",
        "    caminho_pred = os.path.join(novo_caminho, arquivo_pred)\n",
        "    pd.DataFrame(predicao, columns=[arquivo]).to_csv(caminho_pred, index=False)\n",
        "\n",
        "    avaliar_segmentos(X, y, predicao, probabilidade, X_enc, avaliar_dataset, arquivo, dicio_atributos_sensiveis.get(arquivo))\n",
        "\n",
        "  else:\n",
        "    print(f'Não foi possível processar o dataset {arquivo}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "NBfYc87aMBWh",
        "outputId": "e7d2ac65-e173-49cb-f2c6-62bfda6bbfa8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.8500967415005681, 'f1_score': 0.7812161456744253, 'roc_auc_score': np.float64(0.8917733222473028)}\n",
            "AVALIAÇÃO DOS SEGMENTOS: Adult.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'dict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1dfa0ea54125>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicao\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marquivo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaminho_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mavaliar_segmentos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicao\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilidade\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavaliar_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marquivo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicio_atributos_sensiveis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marquivo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-89bb38dd4abc>\u001b[0m in \u001b[0;36mavaliar_segmentos\u001b[0;34m(X, y, predicao, probabilidade, X_enc, avaliar_dataset_func, dataset, atributos_sensiveis)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcoluna_original\u001b[0m \u001b[0;32min\u001b[0m \u001b[0matributos_sensiveis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcoluna_original\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcoluna_original\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_enc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Atributo sensível {coluna_original} não encontrado para {dataset}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5356\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5357\u001b[0m         \"\"\"\n\u001b[0;32m-> 5358\u001b[0;31m         \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5359\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Colunas categóricas\n",
        "if any(col.startswith('V2_') for col in X.columns):\n",
        "  idx = X[\"V2_Female\"] == True\n",
        "  print(avaliar_dataset(y[idx], predicao[idx], probabilidade[idx]))\n",
        "  idx = X[\"V2_Male\"] == True\n",
        "  print(avaliar_dataset(y[idx], predicao[idx], probabilidade[idx]))\n",
        "else:\n",
        "  print(f\"Coluna não encontrada\")\n",
        "\n",
        "idx = X['sex'] == 'male'\n",
        "avaliar_dataset(y[idx], predicao[idx], probabilidade[idx])\n",
        "idx = X['sex'] == 'female'\n",
        "avaliar_dataset(y[idx], predicao[idx], probabilidade[idx])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "i6IRK3A4t0LO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "46e7d5ff-732d-4dca-ca96-709fb2100ed0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Colunas categóricas\\nif any(col.startswith(\\'V2_\\') for col in X.columns):\\n  idx = X[\"V2_Female\"] == True\\n  print(avaliar_dataset(y[idx], predicao[idx], probabilidade[idx]))\\n  idx = X[\"V2_Male\"] == True\\n  print(avaliar_dataset(y[idx], predicao[idx], probabilidade[idx]))\\nelse:\\n  print(f\"Coluna não encontrada\")\\n\\nidx = X[\\'sex\\'] == \\'male\\'\\navaliar_dataset(y[idx], predicao[idx], probabilidade[idx])\\nidx = X[\\'sex\\'] == \\'female\\'\\navaliar_dataset(y[idx], predicao[idx], probabilidade[idx])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colunas = ['NOME DO DATASET', 'MODELO', 'ACURÁCIA', 'F1', 'AUC']\n",
        "print(tabulate(res_metrica_tabela, headers=colunas, tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_rQcVXMOovg",
        "outputId": "d9a112f1-dd32-4aa3-9e0e-a1374b088d32"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒═══════════════════╤═════════════════════════════════════════╤════════════╤══════════╤══════════╕\n",
            "│ NOME DO DATASET   │ MODELO                                  │   ACURÁCIA │       F1 │      AUC │\n",
            "╞═══════════════════╪═════════════════════════════════════════╪════════════╪══════════╪══════════╡\n",
            "│ Adult.csv         │ RandomForestClassifier(random_state=42) │   0.853905 │ 0.788886 │ 0.902509 │\n",
            "╘═══════════════════╧═════════════════════════════════════════╧════════════╧══════════╧══════════╛\n"
          ]
        }
      ]
    }
  ]
}